{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aacbe36b-1c89-473b-8926-7c55dac753af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grup yang paling aktif :  Peserta Fakultaria 2022\n"
     ]
    }
   ],
   "source": [
    "# Mengonversi file teks ke format CSV dan mengekstraknya ke dalam file tar\n",
    "import re\n",
    "import csv \n",
    "import tarfile\n",
    "from collections import defaultdict\n",
    "\n",
    "# Membaca kembali file teks\n",
    "with open(\"chat.txt\", 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "group_comments = defaultdict(list)\n",
    "current_group = None\n",
    "\n",
    "for line in lines:\n",
    "    # Deteksi pembuatan grup baru\n",
    "    group_match = re.search(r'membuat grup \"(.*?)\"', line)\n",
    "    if group_match:\n",
    "        current_group = group_match.group(1)\n",
    "    \n",
    "    # Menyimpan komentar ke grup saat ini\n",
    "    if current_group and re.search(r':', line):\n",
    "        group_comments[current_group].append(line)\n",
    "\n",
    "# Menentukan grup dengan komentar terbanyak\n",
    "most_active_group = max(group_comments, key=lambda x: len(group_comments[x]))\n",
    "most_active_comments = group_comments[most_active_group]\n",
    "\n",
    "print(\"Grup yang paling aktif : \",most_active_group)\n",
    "\n",
    "# Menyimpan dalam format CSV\n",
    "csv_file = \"data_group.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Timestamp\", \"Sender\", \"Message\"])\n",
    "    \n",
    "    for line in lines:\n",
    "        match = re.match(r'^(.*?) - (.*?): (.*)', line)\n",
    "        if match:\n",
    "            timestamp, sender, message = match.groups()\n",
    "            writer.writerow([timestamp, sender, message])\n",
    "\n",
    "tar_file = \"data_group.tar\"\n",
    "with tarfile.open(tar_file, \"w\") as tar:\n",
    "    tar.add(csv_file, arcname=\"data_group.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3f830f-ca15-4536-aefe-2728c2f681a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_csv_to_xlsx(input_csv: str, output_xlsx: str):\n",
    "    \"\"\"\n",
    "    Converts a CSV file to an XLSX file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv: Path to the input CSV file.\n",
    "    - output_xlsx: Path to the output XLSX file.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(input_csv)\n",
    "    print(f\"CSV file '{input_csv}' read successfully with {len(data)} rows and {len(data.columns)} columns.\")\n",
    "    \n",
    "    # Save as XLSX\n",
    "    data.to_excel(output_xlsx, index=False)\n",
    "    print(f\"Data has been saved to '{output_xlsx}'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "606b542a-4060-4a1c-949d-06419287b9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'data_group.csv' read successfully with 3168 rows and 3 columns.\n",
      "Data has been saved to 'data_group.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "convert_csv_to_xlsx('data_group.csv', 'data_group.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c322f7f-bcde-458f-890c-aa07ce4f946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def scale_and_cluster_data(input_file, output_file, n_clusters):\n",
    "    # Read data from XLSX file\n",
    "    data = pd.read_excel(input_file)\n",
    "    print(f\"Data read successfully with {len(data)} rows and {len(data.columns)} columns.\")\n",
    "    \n",
    "    # Handle missing values in the 'message' column\n",
    "    if 'message' not in data.columns:\n",
    "        raise ValueError(\"The input file does not have a 'message' column.\")\n",
    "    data['message'] = data['message'].fillna(\"\")  # Replace NaN with empty strings\n",
    "    \n",
    "    # Convert text data to numerical format using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    vectorized_data = vectorizer.fit_transform(data['message'])\n",
    "    print(f\"Vectorized data shape: {vectorized_data.shape}\")\n",
    "    \n",
    "    # Perform clustering with KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    data['cluster'] = kmeans.fit_predict(vectorized_data)\n",
    "    print(f\"Clustering completed with {n_clusters} clusters.\")\n",
    "    \n",
    "    # Save clustered data to a new XLSX file\n",
    "    data.to_excel(output_file, index=False)\n",
    "    print(f\"Clustered data saved to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ad4e84-5806-4cce-9fbc-dcad3cd01dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read successfully with 3168 rows and 3 columns.\n",
      "Vectorized data shape: (3168, 3504)\n",
      "Clustering completed with 5 clusters.\n",
      "Clustered data saved to 'clustered_data.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "scale_and_cluster_data('data_group.xlsx', 'clustered_data.xlsx', n_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3e5d6f-8125-44a8-9857-6668341e563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def scale_and_cluster_data_with_top_words(input_file, output_file, n_clusters_list):\n",
    "    # Read data from XLSX file\n",
    "    data = pd.read_excel(input_file)\n",
    "    print(f\"Data read successfully with {len(data)} rows and {len(data.columns)} columns.\")\n",
    "    \n",
    "    # Handle missing values in the 'message' column\n",
    "    if 'message' not in data.columns:\n",
    "        raise ValueError(\"The input file does not have a 'message' column.\")\n",
    "    data['message'] = data['message'].fillna(\"\")  # Replace NaN with empty strings\n",
    "    \n",
    "    # Convert text data to numerical format using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    vectorized_data = vectorizer.fit_transform(data['message'])\n",
    "    print(f\"Vectorized data shape: {vectorized_data.shape}\")\n",
    "    \n",
    "    top_words = {}\n",
    "    \n",
    "    for n_clusters in n_clusters_list:\n",
    "        # Perform clustering with KMeans\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        data[f'cluster_{n_clusters}'] = kmeans.fit_predict(vectorized_data)\n",
    "        print(f\"Clustering completed with {n_clusters} clusters.\")\n",
    "        \n",
    "        # Extract top words for each cluster\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        top_words[n_clusters] = [\n",
    "            [feature_names[i] for i in cluster_center.argsort()[-3:][::-1]]\n",
    "            for cluster_center in cluster_centers\n",
    "        ]\n",
    "    \n",
    "    # Save clustered data to a new XLSX file\n",
    "    data.to_excel(output_file, index=False)\n",
    "    print(f\"Clustered data saved to '{output_file}'.\")\n",
    "    \n",
    "    return top_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3173f442-d01c-45cb-a8a6-cd86ec250745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read successfully with 3168 rows and 3 columns.\n",
      "Vectorized data shape: (3168, 3504)\n",
      "Clustering completed with 3 clusters.\n",
      "Clustering completed with 4 clusters.\n",
      "Clustering completed with 5 clusters.\n",
      "Clustered data saved to 'clustered_data.xlsx'.\n",
      "Top words for 3 clusters:\n",
      "Cluster 0: ['azka', 'losss', 'knp']\n",
      "Cluster 1: ['disertakan', 'media', 'tidak']\n",
      "Cluster 2: ['kak', 'waalaikumsalam', 'ini']\n",
      "\n",
      "Top words for 4 clusters:\n",
      "Cluster 0: ['azka', 'losss', 'knp']\n",
      "Cluster 1: ['disertakan', 'media', 'tidak']\n",
      "Cluster 2: ['kak', 'waalaikumsalam', 'ini']\n",
      "Cluster 3: ['aku', 'ya', 'info']\n",
      "\n",
      "Top words for 5 clusters:\n",
      "Cluster 0: ['azka', 'losss', 'knp']\n",
      "Cluster 1: ['disertakan', 'media', 'tidak']\n",
      "Cluster 2: ['kak', 'waalaikumsalam', 'ini']\n",
      "Cluster 3: ['aku', 'ya', 'info']\n",
      "Cluster 4: ['wa', 'alaikumussalam', 'alaikumsalam']\n"
     ]
    }
   ],
   "source": [
    "# Cluster and extract top words\n",
    "top_words = scale_and_cluster_data_with_top_words(\n",
    "    input_file='data_group.xlsx',\n",
    "    output_file='clustered_data.xlsx',\n",
    "    n_clusters_list=[3, 4, 5]\n",
    ")\n",
    "\n",
    "# Display top words for 3, 4, and 5 clusters\n",
    "k3 = top_words[3]\n",
    "k4 = top_words[4]\n",
    "k5 = top_words[5]\n",
    "\n",
    "# Print top 3 words from each cluster\n",
    "print(\"Top words for 3 clusters:\")\n",
    "for i, words in enumerate(k3):\n",
    "    print(f\"Cluster {i}: {words[:3]}\")\n",
    "\n",
    "print(\"\\nTop words for 4 clusters:\")\n",
    "for i, words in enumerate(k4):\n",
    "    print(f\"Cluster {i}: {words[:3]}\")\n",
    "\n",
    "print(\"\\nTop words for 5 clusters:\")\n",
    "for i, words in enumerate(k5):\n",
    "    print(f\"Cluster {i}: {words[:3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fe31ecc-a3b9-4526-8963-719fa2b20ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for 3 clusters:\n",
      "['kak', 'malam', 'malam kak']\n",
      "['pesan', 'ya', 'dihapus']\n",
      "['disertakan', 'media disertakan', 'media']\n",
      "\n",
      "Top words for 4 clusters:\n",
      "['senin', 'tm imm', 'temanÂ²']\n",
      "['disertakan', 'media disertakan', 'media']\n",
      "['kak', 'ya', 'pesan']\n",
      "['malam kak', 'malam', 'waalaikumsalam malam']\n",
      "\n",
      "Top words for 5 clusters:\n",
      "Cluster 0: ['ya', 'info', 'ga']\n",
      "Cluster 1: ['disertakan', 'media disertakan', 'media']\n",
      "Cluster 2: ['pesan dihapus', 'dihapus', 'pesan']\n",
      "Cluster 3: ['waalaikumsalam', 'waalaikumsalam malam', 'malam kak']\n",
      "Cluster 4: ['kak', 'waalaikumussalam', 'malam kak']\n"
     ]
    }
   ],
   "source": [
    "from metaflow import Flow\n",
    "\n",
    "# Mengambil hasil dari workflow terakhir\n",
    "run = Flow('ManyKmeansFlow').latest_run\n",
    "\n",
    "# Mengambil hasil clustering untuk 3, 4, dan 5 cluster\n",
    "k3 = run.data.top[3]\n",
    "k4 = run.data.top[4]\n",
    "k5 = run.data.top[5]\n",
    "\n",
    "# Menampilkan 3 kata teratas untuk setiap cluster\n",
    "print(\"Top words for 3 clusters:\")\n",
    "print(k3[0][:3])  # Cluster pertama\n",
    "print(k3[1][:3])  # Cluster kedua\n",
    "print(k3[2][:3])  # Cluster ketiga\n",
    "\n",
    "print(\"\\nTop words for 4 clusters:\")\n",
    "print(k4[0][:3])  # Cluster pertama\n",
    "print(k4[1][:3])  # Cluster kedua\n",
    "print(k4[2][:3])  # Cluster ketiga\n",
    "print(k4[3][:3])  # Cluster keempat\n",
    "\n",
    "print(\"\\nTop words for 5 clusters:\")\n",
    "for i in range(5):\n",
    "    print(f\"Cluster {i}: {k5[i][:3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c316ff-4864-4a0d-b9d3-2dc83123d8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
